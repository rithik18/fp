{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f0fSd9hTayZ"
      },
      "source": [
        "# **Data Pre-Processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ONNvJFS1xKA2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fm5g0HsiygH0"
      },
      "outputs": [],
      "source": [
        "role_skill=pd.read_csv('role_skill.csv')\n",
        "user_certification=pd.read_csv('user_certification.csv')\n",
        "user_skill=pd.read_csv('user_skill.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXtBGyS9yyvz",
        "outputId": "2243cdcf-cb3f-4bd8-9ab5-62c1a39c0f2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(150, 19)\n",
            "(0, 30)\n",
            "(300000, 23)\n"
          ]
        }
      ],
      "source": [
        "print(role_skill.shape)\n",
        "print(user_certification.shape)\n",
        "print(user_skill.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxtlpPrhy3UD",
        "outputId": "764cc414-623b-4ec0-8ab7-a6e736a43051"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['Unnamed: 0', 'roleId_x', 'skillId', 'name_x', 'role_name',\n",
            "       'skill_name', '_id_x', 'name_y', 'desc_x', 'created_at_x',\n",
            "       'updated_at_x', 'roleId_y', 'skill_count', '_id_y', 'name',\n",
            "       'created_at_y', 'updated_at_y', 'desc_y', 'user_count'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(role_skill.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMw4k72ozBPT",
        "outputId": "51a1f1d2-5bda-4ffa-ed2a-4b5d8745bf2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['Unnamed: 0.1', '_id_x', 'userId_x', 'certificationId',\n",
            "       'certificationName', 'started_at', 'completed_at', 'competency',\n",
            "       'isVerified', 'imageData', 'user_name', 'total_duration', 'count',\n",
            "       'name_x', 'role_id', 'joining_date', 'department', 'mail', 'created_at',\n",
            "       'updated_at', 'password', 'profileImage', 'role_name', 'Unnamed: 0',\n",
            "       '_id', 'name_y', 'issued_by', 'is_certificate', 'role', 'user_count'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(user_certification.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0e8iDKCzoej",
        "outputId": "d586d8a0-eaf9-44a1-acf8-b519156d48ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['Unnamed: 0.1', '_id_x', 'userId_x', 'skillId', 'score', 'user_name',\n",
            "       'skill_name', 'total_duration', 'count', 'name_x', 'role_id',\n",
            "       'joining_date', 'department', 'mail', 'created_at_x', 'updated_at_x',\n",
            "       'password', 'profileImage', 'role_name', 'Unnamed: 0', '_id', 'desc',\n",
            "       'user_count'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(user_skill.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDDXaMku1LO_"
      },
      "source": [
        "NEW_TRIAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_1C696EO2rnl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKnKEIh52nEN",
        "outputId": "8f12fae2-7417-4182-cd20-a745ea7b44c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dynamic Competency Mapping: {}\n",
            "Empty DataFrame\n",
            "Columns: [competency]\n",
            "Index: []\n",
            "(0, 1)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "at least one array or dtype is required",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Standardize the data for clustering\u001b[39;00m\n\u001b[0;32m     26\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m---> 27\u001b[0m scaled_user_cert_matrix \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(user_cert_matrix)\n",
            "File \u001b[1;32mc:\\Users\\RithikHarendarMahesh\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
            "File \u001b[1;32mc:\\Users\\RithikHarendarMahesh\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1084\u001b[0m             (\n\u001b[0;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1094\u001b[0m         )\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
            "File \u001b[1;32mc:\\Users\\RithikHarendarMahesh\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:876\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 876\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_fit(X, y, sample_weight)\n",
            "File \u001b[1;32mc:\\Users\\RithikHarendarMahesh\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\RithikHarendarMahesh\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:912\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    881\u001b[0m \n\u001b[0;32m    882\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    911\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 912\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    913\u001b[0m     X,\n\u001b[0;32m    914\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    915\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mFLOAT_DTYPES,\n\u001b[0;32m    916\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    917\u001b[0m     reset\u001b[38;5;241m=\u001b[39mfirst_call,\n\u001b[0;32m    918\u001b[0m )\n\u001b[0;32m    919\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\RithikHarendarMahesh\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
            "File \u001b[1;32mc:\\Users\\RithikHarendarMahesh\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    875\u001b[0m pandas_requires_conversion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    876\u001b[0m     _pandas_dtype_needs_early_conversion(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m dtypes_orig\n\u001b[0;32m    877\u001b[0m )\n\u001b[0;32m    878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(dtype_iter, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;28;01mfor\u001b[39;00m dtype_iter \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[1;32m--> 879\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mresult_type(\u001b[38;5;241m*\u001b[39mdtypes_orig)\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pandas_requires_conversion \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(d \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;66;03m# Force object if any of the dtypes is an object\u001b[39;00m\n\u001b[0;32m    882\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\n",
            "\u001b[1;31mValueError\u001b[0m: at least one array or dtype is required"
          ]
        }
      ],
      "source": [
        "unique_competency_levels = user_certification['competency'].unique()\n",
        "competency_mapping = {level: i+1 for i, level in enumerate(sorted(unique_competency_levels))}\n",
        "print(\"Dynamic Competency Mapping:\", competency_mapping)\n",
        "\n",
        "# Apply mapping to 'competency' column\n",
        "user_certification['competency'] = user_certification['competency'].map(competency_mapping)\n",
        "\n",
        "# Handle missing values after mapping\n",
        "# user_certification['competency'].fillna(0, inplace=True)\n",
        "\n",
        "# Verify the transformation\n",
        "print(user_certification[['competency']].head())\n",
        "print(user_certification[['competency']].shape)\n",
        "\n",
        "# Check for duplicates and remove if necessary\n",
        "user_certification = user_certification.drop_duplicates()\n",
        "\n",
        "# ------------- Feature Engineering --------------\n",
        "# Select relevant features for clustering (e.g., user_id and their certification details)\n",
        "features = user_certification[['userId_x', 'certificationId', 'competency']]\n",
        "\n",
        "# Pivot the data to create a user-certification matrix\n",
        "user_cert_matrix = features.pivot_table(index='userId_x', columns='certificationId', values='competency', fill_value=0)\n",
        "\n",
        "# Standardize the data for clustering\n",
        "scaler = StandardScaler()\n",
        "scaled_user_cert_matrix = scaler.fit_transform(user_cert_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "oZJ4OrRr4ks0"
      },
      "outputs": [],
      "source": [
        "user_id = '0f988104-a7b9-43e6-8cd1-1cb63d1a68fa'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x1QU33DSHwO"
      },
      "source": [
        "# **KNN Role Based recommendation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POGA-RJJ1M7O",
        "outputId": "79a27ee9-89e8-4b34-969e-575257f22f49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                               userId_x  skill_count  certification_count\n",
            "0  0001cd17-b1ef-481e-8f16-b86c03222af3           10                    5\n",
            "1  000897ae-ccc9-469e-92d6-e3ed79b4b369           10                    5\n",
            "2  000a146b-94c2-45e6-99ac-eb62ac2b022e           10                    5\n",
            "3  000b0fad-ede6-4786-bee6-b8cefba98cf3           10                    5\n",
            "4  000b87bb-2672-49e4-a4c5-ceb032b0395c           10                    5\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encode categorical columns\n",
        "le_role = LabelEncoder()\n",
        "user_skill['role_id'] = le_role.fit_transform(user_skill['role_id'])\n",
        "role_skill['RoleId'] = le_role.fit_transform(role_skill['roleId_x'])\n",
        "\n",
        "# Summarize user certifications\n",
        "user_cert_summary = user_certification.groupby('userId_x')['certificationId'].count().reset_index()\n",
        "user_cert_summary.columns = ['userId_x', 'certification_count']\n",
        "\n",
        "# Summarize user skills\n",
        "user_skill_summary = user_skill.groupby('userId_x')['skillId'].count().reset_index()\n",
        "user_skill_summary.columns = ['userId_x', 'skill_count']\n",
        "\n",
        "# Merge with user_certifications\n",
        "user_features = pd.merge(user_skill_summary, user_cert_summary, on='userId_x', how='left').fillna(0)\n",
        "\n",
        "# Display the user features\n",
        "print(user_features.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uWBzcbc6MEa",
        "outputId": "c248ca3f-d6f7-4e56-859a-39e786b0fe85"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\RithikHarendarMahesh\\AppData\\Local\\Temp\\ipykernel_6512\\316703717.py:21: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  user_features.fillna(0, inplace=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are still NaN values. Investigating further.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Input X contains NaN.\nNearestNeighbors does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo NaN values found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Apply KNN clustering (set n_neighbors to the number of similar users to find)\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m knn \u001b[38;5;241m=\u001b[39m NearestNeighbors(n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, algorithm\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mfit(user_features)  \u001b[38;5;66;03m# Assuming we want to find 5 similar users\u001b[39;00m\n\u001b[0;32m     41\u001b[0m distances, indices \u001b[38;5;241m=\u001b[39m knn\u001b[38;5;241m.\u001b[39mkneighbors(user_features)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Recommend certifications based on similar users\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\RithikHarendarMahesh\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\RithikHarendarMahesh\\anaconda3\\Lib\\site-packages\\sklearn\\neighbors\\_unsupervised.py:175\u001b[0m, in \u001b[0;36mNearestNeighbors.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;66;03m# NearestNeighbors.metric is not validated yet\u001b[39;00m\n\u001b[0;32m    156\u001b[0m     prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    157\u001b[0m )\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    159\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the nearest neighbors estimator from the training dataset.\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \n\u001b[0;32m    161\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;124;03m        The fitted nearest neighbors estimator.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X)\n",
            "File \u001b[1;32mc:\\Users\\RithikHarendarMahesh\\anaconda3\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:518\u001b[0m, in \u001b[0;36mNeighborsBase._fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, (KDTree, BallTree, NeighborsBase)):\n\u001b[1;32m--> 518\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_algorithm_metric()\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\RithikHarendarMahesh\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
            "File \u001b[1;32mc:\\Users\\RithikHarendarMahesh\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1049\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1043\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1044\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1045\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1046\u001b[0m     )\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m-> 1049\u001b[0m     _assert_all_finite(\n\u001b[0;32m   1050\u001b[0m         array,\n\u001b[0;32m   1051\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m   1052\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m   1053\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1054\u001b[0m     )\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1057\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1058\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\RithikHarendarMahesh\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:126\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m _assert_all_finite_element_wise(\n\u001b[0;32m    127\u001b[0m     X,\n\u001b[0;32m    128\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[0;32m    129\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[0;32m    130\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[0;32m    131\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    132\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    133\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\RithikHarendarMahesh\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:175\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    174\u001b[0m     )\n\u001b[1;32m--> 175\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
            "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nNearestNeighbors does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load user_skill, role_skill, and user_certification data\n",
        "user_skill_df = pd.read_csv(\"user_skill.csv\")\n",
        "role_skill_df = pd.read_csv(\"role_skill.csv\")\n",
        "user_certification_df = pd.read_csv(\"user_certification.csv\")\n",
        "\n",
        "# Create user-skill matrix\n",
        "user_skill_matrix = user_skill_df.pivot_table(index='userId_x', columns='skill_name', values='score', fill_value=0)\n",
        "\n",
        "# Map users to roles (optional, if needed for further analysis)\n",
        "user_role_mapping = user_certification_df[['userId_x', 'role_name']]\n",
        "\n",
        "# Merge user skills with role information\n",
        "user_features = user_skill_matrix.merge(user_role_mapping, left_index=True, right_on='userId_x', how='left')\n",
        "\n",
        "# Fill missing values\n",
        "user_features.fillna(0, inplace=True)\n",
        "\n",
        "# One-hot encode the 'role_name' column to convert it to numeric values\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "role_encoded = encoder.fit_transform(user_features[['role_name']])\n",
        "\n",
        "# Create a new DataFrame with the encoded roles\n",
        "role_encoded_df = pd.DataFrame(role_encoded, columns=encoder.get_feature_names_out(['role_name']))\n",
        "\n",
        "# Concatenate the original features with the encoded role features\n",
        "user_features = pd.concat([user_features.drop(columns=['role_name', 'userId_x']), role_encoded_df], axis=1)\n",
        "\n",
        "# Make sure there are no remaining NaN values\n",
        "if user_features.isnull().any().any():\n",
        "    print(\"There are still NaN values. Investigating further.\")\n",
        "else:\n",
        "    print(\"No NaN values found.\")\n",
        "\n",
        "# Apply KNN clustering (set n_neighbors to the number of similar users to find)\n",
        "knn = NearestNeighbors(n_neighbors=5, algorithm='auto').fit(user_features)  # Assuming we want to find 5 similar users\n",
        "distances, indices = knn.kneighbors(user_features)\n",
        "\n",
        "# Recommend certifications based on similar users\n",
        "def recommend_certifications(user_id, top_n=5):\n",
        "    # Ensure the user ID exists in the dataset\n",
        "    if user_id not in user_role_mapping['userId_x'].values:\n",
        "        print(f\"User ID {user_id} not found.\")\n",
        "        return []\n",
        "\n",
        "    # Find the index of the user in the user features DataFrame\n",
        "    user_index_list = user_role_mapping.index[user_role_mapping['userId_x'] == user_id].tolist()\n",
        "    \n",
        "    if len(user_index_list) == 0:\n",
        "        print(f\"User ID {user_id} not found in user features.\")\n",
        "        return []\n",
        "    \n",
        "    user_index = user_index_list[0]\n",
        "    \n",
        "    # Get the indices of the nearest neighbors (excluding the user themselves)\n",
        "    similar_user_indices = indices[user_index][1:]  # Exclude the first element (which is the user itself)\n",
        "    \n",
        "    # Get the certifications of similar users\n",
        "    similar_users = user_certification_df[user_certification_df['userId_x'].isin(user_role_mapping.iloc[similar_user_indices]['userId_x'])]\n",
        "    \n",
        "    # Recommend the most common certifications among similar users\n",
        "    recommended_certifications = similar_users['certification_name'].value_counts().head(top_n).index.tolist()\n",
        "    \n",
        "    return recommended_certifications\n",
        "\n",
        "# Example recommendation\n",
        "user_id = '4cae88bd-a7b8-4090-a135-606f4046e89e'  # Replace with an actual user ID from your dataset\n",
        "recommended_certifications = recommend_certifications(user_id)\n",
        "print(\"Recommended Certifications for User:\", recommended_certifications)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recommended Certifications for User: ['Certified Ethical Hacker', 'CompTIA Security+', 'Certified Information Systems Security Professional', 'Certified Cloud Security Professional', 'CompTIA Cybersecurity Analyst']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load user_skill, role_skill, and user_certification data\n",
        "user_skill_df = pd.read_csv(\"user_skill.csv\")\n",
        "role_skill_df = pd.read_csv(\"role_skill.csv\")\n",
        "user_certification_df = pd.read_csv(\"user_certification.csv\")\n",
        "\n",
        "# Create user-skill matrix\n",
        "user_skill_matrix = user_skill_df.pivot_table(index='userId_x', columns='skill_name', values='score', fill_value=0)\n",
        "\n",
        "# Create role-skill matrix\n",
        "role_skill_matrix = role_skill_df.pivot_table(index='role_name', columns='skill_name', values='skill_count', fill_value=0)\n",
        "\n",
        "# Map users to roles\n",
        "user_role_mapping = user_certification_df[['userId_x', 'role_name']]\n",
        "\n",
        "# Merge user skills with role information\n",
        "user_features = user_skill_matrix.merge(user_role_mapping, left_index=True, right_on='userId_x', how='left')\n",
        "\n",
        "# Fill missing values\n",
        "user_features.fillna(0, inplace=True)\n",
        "\n",
        "# One-hot encode the 'role_name' column to convert it to numeric values\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "role_encoded = encoder.fit_transform(user_features[['role_name']])\n",
        "\n",
        "# Create a new DataFrame with the encoded roles\n",
        "role_encoded_df = pd.DataFrame(role_encoded, columns=encoder.get_feature_names_out(['role_name']))\n",
        "\n",
        "# Concatenate the original features with the encoded role features\n",
        "user_features = pd.concat([user_features.drop(columns=['role_name', 'userId_x']), role_encoded_df], axis=1)  # Exclude 'userId_x'\n",
        "\n",
        "# Apply KNN clustering (set n_neighbors based on the number of unique roles)\n",
        "nbrs = NearestNeighbors(n_neighbors=user_certification_df['role_name'].nunique(), algorithm='auto').fit(user_features)  # Ensure numeric data\n",
        "distances, indices = nbrs.kneighbors(user_features)\n",
        "\n",
        "# Recommend certifications based on user role\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recommended Certifications for User: ['Google Data Analytics Professional Certificate', 'AWS Certified Machine Learning - Specialty', 'IBM Certified Data Scientist', 'Certified Analytics Professional', 'Certified Ethical Hacker']\n"
          ]
        }
      ],
      "source": [
        "def recommend_certifications(user_id, top_n=5):\n",
        "    # Get the user's role\n",
        "    user_role = user_role_mapping[user_role_mapping['userId_x'] == user_id]['role_name'].values[0]\n",
        "    \n",
        "    # Get certifications for that role (assuming role_certification_schema is defined elsewhere)\n",
        "    certifications = role_certification_schema.get(user_role, [])\n",
        "    \n",
        "    return certifications[:top_n]\n",
        "\n",
        "# Example recommendation\n",
        "user_id = 'd8880c16-445c-4985-871d-2dc2c0777803'  # Replace with an actual user ID\n",
        "recommended_certifications = recommend_certifications(user_id)\n",
        "print(\"Recommended Certifications for User:\", recommended_certifications)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt5BvK_7167-"
      },
      "source": [
        "Collaborative Filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGlqy4hl1wuO",
        "outputId": "b8ffd73f-3fbf-4b80-d4fd-033f13e91088"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User ID 0f988104-a7b9-43e6-8cd1-1cb63d1a68fa not found.\n",
            "Recommended Certifications: []\n"
          ]
        }
      ],
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Apply SVD\n",
        "user_item_matrix = user_certification.pivot_table(index='userId_x', columns='certificationId',\n",
        "                                                   values='competency', fill_value=0)\n",
        "\n",
        "svd = TruncatedSVD(n_components=10)\n",
        "user_item_matrix_svd = svd.fit_transform(user_item_matrix)\n",
        "\n",
        "# Cosine similarity between users\n",
        "user_similarity = cosine_similarity(user_item_matrix_svd)\n",
        "\n",
        "# Recommend certifications based on similar users\n",
        "def recommend_collaborative(user_id, top_n=5):\n",
        "    try:\n",
        "        user_idx = user_item_matrix.index.get_loc(user_id)  # Get user index\n",
        "    except KeyError:\n",
        "        print(f\"User ID {user_id} not found.\")\n",
        "        return []\n",
        "\n",
        "    similar_users = user_similarity[user_idx].argsort()[-top_n:][::-1]  # Get indices of similar users\n",
        "\n",
        "    # Get certifications done by similar users\n",
        "    similar_user_ids = user_item_matrix.index[similar_users]\n",
        "    recommended_certifications = user_certification[user_certification['userId_x'].isin(similar_user_ids)]\n",
        "\n",
        "    # Return top N unique certification names\n",
        "    return recommended_certifications['certificationName'].value_counts().head(top_n)\n",
        "\n",
        "# Example usage\n",
        "user_id = '0f988104-a7b9-43e6-8cd1-1cb63d1a68fa'  # Replace with an actual user ID\n",
        "recommended_certifications = recommend_collaborative(user_id)\n",
        "print(\"Recommended Certifications:\", recommended_certifications)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "04EU2mKs9b8u",
        "outputId": "93593d37-dc82-4900-8df6-1d26d318eb28"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a user-item matrix (users and their certifications)\n",
        "user_item_matrix = user_certification.pivot_table(index='userId_x', columns='certificationId', values='competency', fill_value=0)\n",
        "\n",
        "# Use SVD to determine the optimal number of components\n",
        "explained_variance = []\n",
        "components_range = range(1, min(user_item_matrix.shape) + 1)\n",
        "\n",
        "for n_components in components_range:\n",
        "    svd = TruncatedSVD(n_components=n_components)\n",
        "    svd.fit(user_item_matrix)\n",
        "    explained_variance.append(svd.explained_variance_ratio_.sum())\n",
        "\n",
        "# Plot the explained variance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(components_range, explained_variance, marker='o')\n",
        "plt.title('Explained Variance by Number of Components')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.grid()\n",
        "plt.axhline(y=0.95, color='r', linestyle='--')  # Line at 95% explained variance\n",
        "plt.show()\n",
        "\n",
        "# Choose the number of components dynamically\n",
        "optimal_n_components = next(x[0] for x in enumerate(explained_variance) if x[1] >= 0.95) + 1\n",
        "\n",
        "# Apply SVD with the optimal number of components\n",
        "svd = TruncatedSVD(n_components=optimal_n_components)\n",
        "user_item_matrix_svd = svd.fit_transform(user_item_matrix)\n",
        "\n",
        "# Cosine similarity between users\n",
        "user_similarity = cosine_similarity(user_item_matrix_svd)\n",
        "\n",
        "# Recommend certifications based on similar users\n",
        "def recommend_collaborative(user_id, top_n=5):\n",
        "    user_idx = user_item_matrix.index.get_loc(user_id)\n",
        "    similar_users = user_similarity[user_idx].argsort()[-top_n:][::-1]\n",
        "\n",
        "    # Get certifications done by similar users\n",
        "    similar_user_ids = user_item_matrix.index[similar_users]\n",
        "    recommended_certifications = user_certification[user_certification['userId_x'].isin(similar_user_ids)]\n",
        "\n",
        "    return recommended_certifications['certificationName'].value_counts().head(top_n)\n",
        "\n",
        "# Example usage\n",
        "user_id = '4cae88bd-a7b8-4090-a135-606f4046e89e'\n",
        "recommended_courses = recommend_collaborative(user_id)\n",
        "print(recommended_courses)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5gyFS1V2Ine"
      },
      "source": [
        "Content based filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjUGQbGIBoPy"
      },
      "source": [
        "Based on Certification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DzstxWj19Uu",
        "outputId": "6466d864-2e1f-4f37-df52-56bd329cde12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No valid certifications found for the user.\n",
            "Recommended Certifications for User: []\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "# Create feature matrix from user certifications\n",
        "# Assuming competency is numeric, otherwise you'll need to convert it to a suitable format\n",
        "certification_features = user_certification[['certificationId', 'competency']].groupby('certificationId').mean()\n",
        "\n",
        "# Cosine similarity for certifications\n",
        "certification_similarity = cosine_similarity(certification_features)\n",
        "\n",
        "# Create a mapping from certificationId to index in the feature matrix\n",
        "certification_index = {certification: index for index, certification in enumerate(certification_features.index)}\n",
        "\n",
        "# Recommend based on content similarity\n",
        "def recommend_content_based(user_id, top_n=5):\n",
        "    user_certifications = user_certification[user_certification['userId_x'] == user_id]['certificationId'].unique()\n",
        "\n",
        "    # Get the indices of user certifications in the similarity matrix\n",
        "    indices = [certification_index[cert] for cert in user_certifications if cert in certification_index]\n",
        "\n",
        "    if not indices:\n",
        "        print(\"No valid certifications found for the user.\")\n",
        "        return []\n",
        "\n",
        "    # Calculate the mean similarity scores for the user's certifications\n",
        "    similar_certs = certification_similarity[indices].mean(axis=0)\n",
        "\n",
        "    # Recommend top certifications\n",
        "    top_cert_indices = similar_certs.argsort()[-top_n:][::-1]\n",
        "    recommended_certifications = certification_features.index[top_cert_indices]\n",
        "\n",
        "    return user_certification[user_certification['certificationId'].isin(recommended_certifications)]['certificationName'].unique()\n",
        "\n",
        "# Example usage\n",
        "# user_id = '0006406b-24cd-4b7b-896a-20e600045c96'  # Replace with an actual user ID\n",
        "recommended_certifications = recommend_content_based(user_id)\n",
        "print(\"Recommended Certifications for User:\", recommended_certifications)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9Hbsa-LDwzC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CdqtjJR0fO1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
